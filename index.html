---
layout: default
title: Home
---

<section class="bio card fade-in-section">
  <h2>üëã Hi, I'm Vardhan Seepala</h2>
  <p><strong>üöÄ Data Engineer</strong> with 5+ years of experience designing enterprise-scale data pipelines and fulfillment solutions across cloud platforms. Proven track record of reducing costs by 30‚Äì50% and accelerating data processing by 40‚Äì70% through scalable infrastructure, ETL/ELT optimizations, and Agile-driven collaboration. Adept at translating business requirements into secure, high-performance data architectures while mentoring teams to adopt best practices.</p>
  <hr>
  
  <h3>What I Do Best ‚ú®</h3>
  <ul>
    <li>Design enterprise-scale data pipelines and fulfillment solutions across cloud platforms.</li>
    <li>Reduce costs by 30‚Äì50% and accelerate data processing by 40‚Äì70% through scalable infrastructure and ETL/ELT optimizations.</li>
    <li>Translate business requirements into secure, high-performance data architectures.</li>
    <li>Mentor teams in adopting best practices for modern data platforms.</li>
  </ul>
</section>

<div class="main-grid">

  <!-- Experience Section -->
  <section class="experience-section card fade-in-section">
    <h2>Experience Snapshot üíº</h2>
    
    <div class="job-entry">
      <h4>Lead Data Engineer @ Viral Nation (Jun 2023 ‚Äì Present)</h4>
      <ul>
        <li>Spearheaded CDC data capture pipelines (Cloud SQL ‚Üí BigQuery) using Datastream (GCP), automating data lake creation and reducing transformation overhead by 40%.</li>
        <li>Deployed Terraform templates to configure Datastream on a private VPC, slashing manual setup time by 50%.</li>
        <li>Engineered DBT models unifying 10+ data sources into marketing-ready analytics layers, reducing data errors by 80% and improving cross-team reporting consistency.</li>
        <li>Architected Dataflow pipelines using Apache Beam (Python SDK) streaming MongoDB/PostgreSQL data to BigQuery and Pub/Sub to BigQuery enabling near real-time decision-making for 10+ client campaigns.</li>
        <li>Built end-to-end CI/CD pipelines for AI model deployment (Azure ML Studio), cutting time-to-market by 35% and operational costs by 25% through Kubernetes-driven scaling.</li>
        <li>Led production deployment of brand detection and profanity filtering models via Kafka consumers, enhancing content moderation for 1M+ daily social media interactions.</li>
        <li>Designed and optimized PySpark-based ETL pipelines in Azure Synapse to aggregate campaign performance data, enabling real-time insights into key metrics (e.g., ROI, engagement rates).</li>
        <li>Collaborated with marketing teams to translate business requirements into custom aggregations, reducing reporting latency by 40% and improving campaign accuracy using Looker Dashboards.</li>
      </ul>
    </div>

    <div class="job-entry">
      <h4>Data Engineer Co-Op @ BDO (Jan 2023 ‚Äì Apr 2023)</h4>
      <ul>
        <li>Redesigned Azure Data Factory pipelines by prioritizing entity dependencies, reducing full-load processing time from 13 to 9 hours (30% cost savings).</li>
        <li>Developed Power BI dashboards integrating MQTT server data with digital twin metrics, enabling real-time production tracking for 50+ manufacturing accounts.</li>
        <li>Authored scripts to automate KQL query generation, accelerating data extraction for Azure Data Explorer by 25%.</li>
      </ul>
    </div>

    <div class="job-entry">
      <h4>Senior Software Engineer @ Oracle Cerner (Aug 2019 ‚Äì Dec 2021)</h4>
      <ul>
          <li>Designed AWS EMR pipelines that reduced processing costs by 30% and runtime by 50% for healthcare data serving 10K+ patients daily.</li>
          <li>Streamlined AWS IAM role creation via CloudFormation, minimizing manual errors by 90% across 15+ client clusters.</li>
          <li>Built automated testing scripts (Java/Python) that cut integration testing time by 70% and reduced post-release bugs by 60%.</li>
          <li>Utilized Java, Python, and microservices to normalize and standardize data based on client needs, meeting international healthcare data standards.</li>
          <li>Led effective communication in Scrum calls and retrospective meetings, identifying and addressing issues that slowed development and deployment progress.</li>
      </ul>
    </div>

    <div class="job-entry">
      <h4>DevOps Engineering Intern @ Sigmoid Pvt Ltd (Apr 2019 ‚Äì Jul 2019)</h4>
      <ul>
          <li>Created Datadog/Elasticsearch dashboards, reducing cluster downtime by 50% through proactive health alerts.</li>
          <li>Delivered client Proof of Concepts that lowered AWS cluster costs by 20%, driving a 15% increase in client acquisition.</li>
      </ul>
    </div>
  </section>

  <!-- Skills Section -->
  <section class="skills-section card fade-in-section">
    <h2>Skills & Tools üõ†Ô∏è</h2>
    <ul>
      <li><i class="fas fa-code"></i> <strong>Languages & Libraries:</strong> Python, Java, SQL, Django, TypeScript, HTML, CSS, JavaScript, Terraform, Apache Beam</li>
      <li><i class="fas fa-database"></i> <strong>Database Systems:</strong> PostgreSQL, MS-SQL Server, KQL, NoSQL, MongoDB</li>
      <li><i class="fas fa-server"></i> <strong>Operating Systems & Containerization:</strong> Linux, OS X, Windows 11, Docker, Kubernetes</li>
      <li><i class="fas fa-cogs"></i> <strong>Frameworks & Orchestration:</strong> Hadoop, Spark, HDFS, HBase, Confluent Kafka, Airflow, Zookeeper, Django, PyTorch</li>
      <li><i class="fas fa-cloud"></i> <strong>Cloud Computing:</strong> Azure, AWS, GCP, Dataflow, BigQuery, Azure ML Studio, Azure SDK</li>
      <li><i class="fas fa-sitemap"></i> <strong>Methodologies:</strong> SDLC (Agile, Scrum)</li>
      <li><i class="fas fa-chart-bar"></i> <strong>Visualization:</strong> Looker, Power BI</li>
    </ul>
  </section>

  <!-- Education Section -->
  <section class="education-section card fade-in-section">
    <h2>Education üéì</h2>
    <ul>
      <li><i class="fas fa-graduation-cap"></i> <strong>Master of Applied Computing</strong> ‚Äì University of Windsor, Canada</li>
      <li><i class="fas fa-graduation-cap"></i> <strong>Bachelor of Engineering in Computer Science</strong> ‚Äì Sir MVIT, India</li>
    </ul>
  </section>

</div>

<section class="data-visualization fade-in-section">
    <h2>My Language Toolkit üìä</h2>
    <div class="card">
        <canvas id="language-chart"></canvas>
    </div>
</section>

<section class="github-repos fade-in-section">
  <h2>Recent GitHub Projects üíª</h2>
  <div id="github-repos-container"></div>
</section>

<section class="latest-posts fade-in-section">
  <h2>Latest Blog Posts üìù</h2>
  <div class="card">
      <ul>
        {% for post in site.posts limit:5 %}
          <li>
            <a href="{{ post.url }}">{{ post.title }}</a>
            <span>{{ post.date | date: "%B %d, %Y" }}</span>
          </li>
        {% endfor %}
      </ul>
  </div>
</section>

<script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
<script>
  async function fetchData() {
    const username = '{{ site.author.github }}';
    const response = await fetch(`https://api.github.com/users/${username}/repos?per_page=100`);
    const repos = await response.json();

    // --- Process for Language Chart ---
    const languageCounts = repos.reduce((acc, repo) => {
        if (repo.language) {
            acc[repo.language] = (acc[repo.language] || 0) + 1;
        }
        return acc;
    }, {});

    const chartData = {
        labels: Object.keys(languageCounts),
        datasets: [{
            label: 'Repositories by Language',
            data: Object.values(languageCounts),
            backgroundColor: '#00a8ff',
            borderColor: '#2c2c2c',
            borderWidth: 2
        }]
    };

    const chartCtx = document.getElementById('language-chart').getContext('2d');
    new Chart(chartCtx, {
        type: 'bar',
        data: chartData,
        options: {
            scales: {
                y: {
                    beginAtZero: true
                }
            }
        }
    });

    // --- Process for Repo Cards ---
    const sortedRepos = repos.sort((a, b) => new Date(b.pushed_at) - new Date(a.pushed_at)).slice(0, 5);
    const container = document.getElementById('github-repos-container');

    sortedRepos.forEach(repo => {
      const repoEl = document.createElement('div');
      repoEl.classList.add('card', 'repo');
      repoEl.innerHTML = `
        <h3><a href="${repo.html_url}" target="_blank">${repo.name}</a></h3>
        <p>${repo.description || 'No description available.'}</p>
        <p class="repo-meta"><strong>Language:</strong> ${repo.language || 'N/A'}</p>
      `;
      container.appendChild(repoEl);
    });
  }

  fetchData();
</script>